{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761abaf6",
   "metadata": {},
   "source": [
    "### import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib.envs.slicing_env import SlicingEnvironment\n",
    "from lib.agents import qlearning\n",
    "from lib import utils\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a2deed",
   "metadata": {},
   "source": [
    "### configure the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6324fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed for reproducibility\n",
    "np.random.seed(2021)\n",
    "\n",
    "# train learner agents using the first batch of reward function weights\n",
    "order = 0\n",
    "\n",
    "# number of DRL agent timesteps per episode \n",
    "max_episode_timesteps = 100\n",
    "\n",
    "total_data_episodes = 1\n",
    "\n",
    "# number of DRL agent episodes (for the sake of better results visulization)\n",
    "total_episodes = 200\n",
    "\n",
    "# qlearning or sarsa\n",
    "# agent_name = 'qlearning'\n",
    "agent_name = 'sarsa'\n",
    "\n",
    "learning_type = 'accelerated'\n",
    "loaded_learning_type = 'non_accelerated'\n",
    "\n",
    "# sigmoid reward function configurations\n",
    "c1_volte = 0.5\n",
    "c2_volte = 10\n",
    "c1_urllc = 2\n",
    "c2_urllc = 3\n",
    "c1_video = 1\n",
    "c2_video = 7\n",
    "\n",
    "# q-learning agent configurations\n",
    "discount_factor=0.3\n",
    "alpha=0.5\n",
    "epsilon=0.1\n",
    "epsilon_decay=0.5\n",
    "decay_steps=20\n",
    "\n",
    "by_alpha=0.1\n",
    "by_epsilon=1\n",
    "by_epsilon_decay=0.9\n",
    "\n",
    "# slicing configurations\n",
    "# number of users per slice in the following order: VoLTE, Video, URLLC\n",
    "num_users = [int(46/4), int(46/4), int(8/4)]\n",
    "\n",
    "poisson_volte = np.full((1, 200), 1)\n",
    "poisson_video = np.full((1, 200), 1)\n",
    "poisson_urllc = np.full((1, 200), 1)\n",
    "\n",
    "max_num_users = [max(poisson_volte[0]), max(poisson_video[0]), max(poisson_urllc[0])]\n",
    "\n",
    "num_users_poisson = [poisson_video[0], poisson_volte[0], poisson_urllc[0]]\n",
    "\n",
    "max_size_per_tti = 40\n",
    "max_num_packets = 0\n",
    "max_traffic_percentage = 1\n",
    "num_action_lvls = 15\n",
    "num_slices = 3\n",
    "sl_win_size = 40\n",
    "time_quantum = 1\n",
    "max_trans_per_tti = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52fbce",
   "metadata": {},
   "source": [
    "### generate sample traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df = utils.generate_data(max_num_users[0], max_num_users[1], \n",
    "                                 max_num_users[2], sl_win_size*max_episode_timesteps)\n",
    "traffic_df = traffic_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087cf1fa",
   "metadata": {},
   "source": [
    "### apply policy reuse to accelerate the learner agents training given the configured reward function weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e671fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    # set the weights of the learner agent's reward function\n",
    "    w_volte = utils.get_reward_weights_acc(i, order)[0]\n",
    "    w_urllc = utils.get_reward_weights_acc(i, order)[1]\n",
    "    w_video = utils.get_reward_weights_acc(i, order)[2]\n",
    "    \n",
    "    for j in range(0, 16):\n",
    "        # set the weights of the expert agent's reward function\n",
    "        by_w_volte = utils.get_reward_weights(j)[0]\n",
    "        by_w_urllc = utils.get_reward_weights(j)[1]\n",
    "        by_w_video = utils.get_reward_weights(j)[2]\n",
    "        \n",
    "        by_file_name = 'saved_models/base/' + str(loaded_learning_type) + '_' +  \\\n",
    "            str(agent_name) + '_' + str(int(by_w_volte*100)) + str(int(by_w_urllc*100)) +  \\\n",
    "            str(int(by_w_video*100)) + '_' + str(by_alpha) + '_' +  \\\n",
    "            str(by_epsilon) + '_' + str(by_epsilon_decay) + '_' + str(total_episodes) + 'ep.npy'\n",
    "\n",
    "        # load the appropriate expert policy\n",
    "        new_dict = np.load(by_file_name, allow_pickle='TRUE')\n",
    "        loaded_qtable = new_dict.item()['qtable']\n",
    "\n",
    "        # initialize the OpenAI gym-compatible environment using the configured simulation parameters\n",
    "        enviro = SlicingEnvironment(traffic_df, max_num_packets, max_size_per_tti, num_action_lvls, \n",
    "                             num_slices, max_episode_timesteps, sl_win_size, time_quantum,total_data_episodes,\n",
    "                             num_users_poisson, max_traffic_percentage, max_trans_per_tti, w_volte, w_urllc,\n",
    "                                w_video, c1_volte, c1_urllc, c1_video, c2_volte, c2_urllc, c2_video)\n",
    "\n",
    "        env = enviro\n",
    "\n",
    "        # start the simulation using a q-learning agent \n",
    "        Q, stats = qlearning.q_learning(env=env, num_episodes=total_episodes, discount_factor=discount_factor,\n",
    "                          alpha=alpha, epsilon=epsilon, epsilon_decay=epsilon_decay,\n",
    "                          decay_steps=decay_steps, loaded_qtable=loaded_qtable)\n",
    "        \n",
    "        # log the trained agents' data\n",
    "        dictionary = {'config': {'generic': {'max_episode_timesteps': max_episode_timesteps, 'total_episodes': total_episodes,\n",
    "                             'agent_name': agent_name, 'max_size_per_tti': max_size_per_tti,\n",
    "                             'max_traffic_percentage': max_traffic_percentage, 'num_action_lvls': num_action_lvls,\n",
    "                             'num_slices': num_slices, 'sl_win_size': sl_win_size, 'max_trans_per_tti': max_trans_per_tti,\n",
    "                             'w_volte': w_volte, 'w_urllc': w_urllc, 'w_video': w_video, 'by_w_volte': by_w_volte, \n",
    "                             'by_w_urllc': by_w_urllc, 'by_w_video': by_w_video,\n",
    "                             'c1_volte': c1_volte,'c2_volte': c2_volte, 'c1_urllc': c1_urllc, 'c2_urllc': c2_urllc,\n",
    "                             'c1_video': c1_video, 'c2_video': c2_video,\n",
    "                             'learning_type': learning_type},\n",
    "                             'agent_specific': {'discount_factor': discount_factor, 'alpha': alpha,\n",
    "                                                'epsilon': epsilon, 'epsilon_decay': epsilon_decay,\n",
    "                                                'decay_steps': decay_steps, 'loaded_qtable': loaded_qtable}\n",
    "                            },\n",
    "                  'rewards': {'steps': env.step_rewards, 'episodes': list(stats[1])},\n",
    "                  'qtable':dict(Q),\n",
    "                  'KPIs': {'delay': env.total_avg_waiting_times,\n",
    "                           'throughput': env.total_throughputs, 'finished_throughput': env.finished_throughputs,\n",
    "                           'remaining_sizes_sum': env.remaining_sizes_sum, 'remaining_sizes': env.remaining_sizes,\n",
    "                           'remaining_times_sum': env.remaining_times_sum, 'remaining_times': env.remaining_times,\n",
    "                           'total_p_numbers': env.total_p_numbers, 'done_p_numbers': env.done_p_numbers\n",
    "                         }}\n",
    "\n",
    "        # save training data to file\n",
    "        path = 'saved_models/accelerated/'\n",
    "        if not os.path.exists(path):\n",
    "          # create a new directory because it does not exist \n",
    "          os.makedirs(path)\n",
    "        file_name = path + str(learning_type) + '_' + str(agent_name) + '_' + str(int(w_volte*100)) + \\\n",
    "                    str(int(w_urllc*100)) + str(int(w_video*100)) + '_by_' + str(int(by_w_volte*100)) + \\\n",
    "                    str(int(by_w_urllc*100)) + str(int(by_w_video*100)) + '_' \\\n",
    "                    + str(alpha) + '_' + str(epsilon) + '_' + str(epsilon_decay) + '_' + str(total_episodes) + 'ep.npy'\n",
    "        np.save(file_name, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ff29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f5b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9716f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04e744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500ef3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a23c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41997258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e21eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4bc433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2a9765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cfa950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc008472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
