{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be24a8e6",
   "metadata": {},
   "source": [
    "### import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f43daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib.envs.slicing_env import SlicingEnvironment\n",
    "from lib.agents import qlearning\n",
    "from lib import utils\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1518d52",
   "metadata": {},
   "source": [
    "### configure the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d711986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed for reproducibility\n",
    "np.random.seed(2021)\n",
    "\n",
    "# train learner agents using the first batch of reward function weights\n",
    "order = 0\n",
    "\n",
    "# number of DRL agent timesteps per episode \n",
    "max_episode_timesteps = 100\n",
    "\n",
    "total_data_episodes = 1\n",
    "\n",
    "# number of DRL agent episodes (for the sake of better results visulization)\n",
    "total_episodes = 200\n",
    "\n",
    "# qlearning or sarsa\n",
    "agent_name = 'qlearning'\n",
    "# agent_name = 'sarsa'\n",
    "\n",
    "learning_type = 'non_accelerated'\n",
    "\n",
    "# sigmoid reward function configurations\n",
    "c1_volte = 0.5\n",
    "c2_volte = 10\n",
    "c1_urllc = 2\n",
    "c2_urllc = 3\n",
    "c1_video = 1\n",
    "c2_video = 7\n",
    "\n",
    "# q-learning agent configurations\n",
    "discount_factor=0.3\n",
    "alpha=0.1\n",
    "epsilon=1\n",
    "epsilon_decay=0.9\n",
    "decay_steps=100\n",
    "\n",
    "# policy reuse flag\n",
    "loaded_qtable='no'\n",
    "\n",
    "# slicing configurations\n",
    "# number of users per slice in the following order: VoLTE, Video, URLLC\n",
    "num_users = [int(46/4), int(46/4), int(8/4)]\n",
    "\n",
    "poisson_volte = np.full((1, 200), 1)\n",
    "poisson_video = np.full((1, 200), 1)\n",
    "poisson_urllc = np.full((1, 200), 1)\n",
    "\n",
    "max_num_users = [max(poisson_volte[0]), max(poisson_video[0]), max(poisson_urllc[0])]\n",
    "\n",
    "num_users_poisson = [poisson_video[0], poisson_volte[0], poisson_urllc[0]]\n",
    "\n",
    "max_size_per_tti = 40 \n",
    "max_num_packets = 0\n",
    "max_traffic_percentage = 1\n",
    "num_action_lvls = 15\n",
    "num_slices = 3\n",
    "sl_win_size = 40\n",
    "time_quantum = 1\n",
    "max_trans_per_tti = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc855713",
   "metadata": {},
   "source": [
    "### generate sample traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df = utils.generate_data(max_num_users[0], max_num_users[1],\n",
    "                                 max_num_users[2], sl_win_size*max_episode_timesteps)\n",
    "traffic_df = traffic_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395a614",
   "metadata": {},
   "source": [
    "### train the learner agents from scratch given the configured reward function weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eafa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    # set the weights of the reward function\n",
    "    w_volte = utils.get_reward_weights_acc(i, order)[0]\n",
    "    w_urllc = utils.get_reward_weights_acc(i, order)[1]\n",
    "    w_video = utils.get_reward_weights_acc(i, order)[2]\n",
    "\n",
    "    # initialize the OpenAI gym-compatible environment using the configured simulation parameters\n",
    "    enviro = SlicingEnvironment(traffic_df, max_num_packets, max_size_per_tti, num_action_lvls, \n",
    "                         num_slices, max_episode_timesteps, sl_win_size, time_quantum,total_data_episodes,\n",
    "                         num_users_poisson, max_traffic_percentage, max_trans_per_tti, w_volte, w_urllc,\n",
    "                            w_video, c1_volte, c1_urllc, c1_video, c2_volte, c2_urllc, c2_video)\n",
    "\n",
    "    env = enviro\n",
    "    \n",
    "    # start the simulation using a q-learning agent \n",
    "    Q, stats = qlearning.q_learning(env=env, num_episodes=total_episodes, discount_factor=discount_factor,\n",
    "                      alpha=alpha, epsilon=epsilon, epsilon_decay=epsilon_decay,\n",
    "                      decay_steps=decay_steps, loaded_qtable=loaded_qtable)\n",
    "    \n",
    "    # log the trained agents' data\n",
    "    dictionary = {'config': {'generic': {'max_episode_timesteps': max_episode_timesteps, 'total_episodes': total_episodes,\n",
    "                         'agent_name': agent_name, 'max_size_per_tti': max_size_per_tti,\n",
    "                         'max_traffic_percentage': max_traffic_percentage, 'num_action_lvls': num_action_lvls,\n",
    "                         'num_slices': num_slices, 'sl_win_size': sl_win_size, 'max_trans_per_tti': max_trans_per_tti,\n",
    "                         'w_volte': w_volte, 'w_urllc': w_urllc, 'w_video': w_video, 'c1_volte': c1_volte,\n",
    "                         'c2_volte': c2_volte, 'c1_urllc': c1_urllc, 'c2_urllc': c2_urllc,\n",
    "                         'c1_video': c1_video, 'c2_video': c2_video, 'learning_type': learning_type},\n",
    "                         'agent_specific': {'discount_factor': discount_factor, 'alpha': alpha,\n",
    "                                            'epsilon': epsilon, 'epsilon_decay': epsilon_decay,\n",
    "                                            'decay_steps': decay_steps, 'loaded_qtable': loaded_qtable}\n",
    "                        },\n",
    "              'rewards': {'steps': env.step_rewards, 'episodes': list(stats[1])},\n",
    "              'qtable':dict(Q),\n",
    "              'KPIs': {'delay': env.total_avg_waiting_times,\n",
    "                       'throughput': env.total_throughputs, 'finished_throughput': env.finished_throughputs,\n",
    "                       'remaining_sizes_sum': env.remaining_sizes_sum, 'remaining_sizes': env.remaining_sizes,\n",
    "                       'remaining_times_sum': env.remaining_times_sum, 'remaining_times': env.remaining_times,\n",
    "                       'total_p_numbers': env.total_p_numbers, 'done_p_numbers': env.done_p_numbers\n",
    "                     }}\n",
    "\n",
    "    # save training data to file\n",
    "    path = 'saved_models/non_accelerated/'\n",
    "    if not os.path.exists(path):\n",
    "      # create a new directory because it does not exist \n",
    "      os.makedirs(path)\n",
    "    file_name = path + str(learning_type) + '_' + str(agent_name) + '_' + str(int(w_volte*100)) + \\\n",
    "                str(int(w_urllc*100)) + str(int(w_video*100)) + '_' + str(alpha) + '_' + str(epsilon) + '_' + str(epsilon_decay) + '_' + str(total_episodes) + 'ep.npy'\n",
    "    np.save(file_name, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b1e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1144b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c76c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85390964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb27b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb26b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913df03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e95b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47cd7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff17bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d033fb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a087220b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18cf3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e78cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1cf64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e865853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf338c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
